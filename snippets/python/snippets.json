{
  "with open read": {
    "prefix": "wor",
    "body": [
      "with open(${1:file_path}, 'r') as ${2:f}:",
      "\t${3:# file operations}"
    ],
    "description": "With statement for reading files"
  },
  "with open write": {
    "prefix": "wow",
    "body": [
      "with open(${1:file_path}, 'w') as ${2:f}:",
      "\t${3:# file operations}"
    ],
    "description": "With statement for writing files"
  },
  "write json file": {
    "prefix": "wjf",
    "body": [
      "with open(${1:path}, 'w', encoding='utf-8') as f:",
      "\tjson.dump(${2:structure}, f, ensure_ascii=False, indent=2)"
    ],
    "description": "Write a json file"
  },
  "patate debug": {
    "prefix": "ptt",
    "body": [
      "print(\"ü•îü•îü•î\", ${1:var})"
    ],
    "description": "patate debug"
  },
  "langchain messages": {
    "prefix": "lcmess",
    "body": [
      "from langchain_core.messages import HumanMessage, SystemMessage"
    ],
    "description": "import langchain messages objects"
  },
  "lambda": {
    "prefix": "lmb",
    "body": [
      "lambda x: ${1:x}"
    ],
    "description": "lambda functiontion"
  },
  "typing": {
    "prefix": "ty",
    "body": "from typing import List, Dict, Optional, Union, Tuple, Any, Callable",
    "description": "import common data structures"
  },
  "Import common data libraries": {
    "prefix": "impdata",
    "body": [
      "import numpy as np",
      "import pandas as pd",
      "import matplotlib.pyplot as plt",
      "import seaborn as sns"
    ],
    "description": "Import common data science libraries"
  },
  "Class definition": {
    "prefix": "cldef",
    "body": [
      "class ${1:ClassName}:",
      "\t\"\"\"${2:Class docstring}\"\"\"",
      "\t",
      "\tdef __init__(self, ${3:parameters}):",
      "\t\t\"\"\"Initialize the ${1:ClassName} instance.",
      "\t\t",
      "\t\tArgs:",
      "\t\t\t${4:param_name}: ${5:param_description}",
      "\t\t\"\"\"",
      "\t\t${6:pass}",
      "\t",
      "\tdef ${7:method_name}(self, ${8:parameters}):",
      "\t\t\"\"\"${9:Method docstring}",
      "\t\t",
      "\t\tArgs:",
      "\t\t\t${10:param_name}: ${11:param_description}",
      "\t\t",
      "\t\tReturns:",
      "\t\t\t${12:return_description}",
      "\t\t\"\"\"",
      "\t\t${13:pass}"
    ],
    "description": "Class definition with methods and docstrings"
  },
  "Try except": {
    "prefix": "tryex",
    "body": [
      "try:",
      "\t${1:# code that might raise an exception}",
      "except ${2:Exception} as ${3:e}:",
      "\t${4:# handle the exception}",
      "\tprint(f\"Error: {${3:e}}\")"
    ],
    "description": "Try-except block"
  },
  "Try except else finally": {
    "prefix": "tryexef",
    "body": [
      "try:",
      "\t${1:# code that might raise an exception}",
      "except ${2:Exception} as ${3:e}:",
      "\t${4:# handle the exception}",
      "\tprint(f\"Error: {${3:e}}\")",
      "else:",
      "\t${5:# code to run if no exception}",
      "finally:",
      "\t${6:# code to run always}"
    ],
    "description": "Try-except-else-finally block"
  },
  "LangChain prompt template": {
    "prefix": "lcpt",
    "body": [
      "from langchain.prompts import PromptTemplate",
      "",
      "# Define a prompt template",
      "prompt_template = PromptTemplate.from_template(",
      "\"\"\"",
      "${1:prompt_text}",
      "Formato richiesto: {${2:format_instructions}}",
      "Input JSON: {${3:input_json}}",
      "\"\"\"",
      ")"
    ],
    "description": "Create a LangChain prompt template"
  },
  "LangChain model": {
    "prefix": "lcm",
    "body": [
      "from langchain.chat_models import init_chat_model",
      "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0.0)"
    ],
    "description": "Initialize a LangChain model"
  },
  "Pydantic model": {
    "prefix": "pydm",
    "body": [
      "from pydantic import BaseModel, Field",
      "from typing import Optional",
      "class ${1:Model}(BaseModel):",
      "\t${2:field_name}: ${3:field_type} = Field(..., description=\"${4:Field description}\")"
    ],
    "description": "Create a Pydantic model"
  },
  "LangChain pydantic parser": {
    "prefix": "lcpp",
    "body": [
      "from langchain.output_parsers import PydanticOutputParser",
      "",
      "${parser} = PydanticOutputParser(pydantic_object=${1:model_name})"
    ],
    "description": "Create a Pydantic output parser for LangChain"
  },
  "LangChain  basic chain": {
    "prefix": "lcc",
    "body": [
      "from langchain.output_parsers import PydanticOutputParser",
      "from langchain.chat_models import init_chat_model",
      "from langchain.prompts import PromptTemplate",
      "",
      "prompt = PromptTemplate.from_template(",
      "\"\"\"",
      "prompt_text",
      "Formato richiesto: {format_instructions}",
      "Input JSON: {input_json}",
      "\"\"\"",
      ")",
      "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0.0)",
      "${parser} = PydanticOutputParser(pydantic_object=model)",
      "similarity_llm = prompt | llm | parser"
    ],
    "description": "Create a Pydantic output parser for LangChain"
  },
  "LangChain RAG setup": {
    "prefix": "lcrag",
    "body": [
      "from langchain.document_loaders import ${1:TextLoader}",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter",
      "from langchain.embeddings import OpenAIEmbeddings",
      "from langchain.vectorstores import Chroma",
      "from langchain.chains import RetrievalQA",
      "from langchain.llms import OpenAI",
      "",
      "# 1. Load documents",
      "loader = ${1:TextLoader}(\"${2:path/to/document}\")",
      "documents = loader.load()",
      "",
      "# 2. Split documents into chunks",
      "text_splitter = RecursiveCharacterTextSplitter(",
      "\tchunk_size=${3:1000},",
      "\tchunk_overlap=${4:100}",
      ")",
      "chunks = text_splitter.split_documents(documents)",
      "",
      "# 3. Create embeddings and vectorstore",
      "embeddings = OpenAIEmbeddings()",
      "vectorstore = Chroma.from_documents(chunks, embeddings)",
      "",
      "# 4. Create a retrieval chain",
      "qa_chain = RetrievalQA.from_chain_type(",
      "\tllm=OpenAI(),",
      "\tchain_type=\"stuff\",",
      "\tretriever=vectorstore.as_retriever()",
      ")",
      "",
      "# 5. Ask a question",
      "result = qa_chain({\"query\": \"${5:Your question about the documents?}\"})"
    ],
    "description": "Set up a Retrieval Augmented Generation (RAG) system with LangChain"
  },
  "Import pathlib": {
    "prefix": "pathlibi",
    "body": [
      "from pathlib import Path"
    ],
    "description": "Import Path from pathlib"
  },
  "cli multimode": {
    "prefix": "cli",
    "body": [
      "import argparse",
      "",
      "def cli():",
      "\tparser = argparse.ArgumentParser(description=\"${1:CLI con pi√π modalit√†}\")",
      "\tparser.add_argument('--parse', type=str, help='Modalit√† parse, nome file da analizzare')",
      "\tparser.add_argument('-a', type=str, help='Modalit√† analisi, nome file da analizzare')",
      "\tparser.add_argument('-b', nargs=2, help='Modalit√† confronto, due nomi di file')",
      "\targs = parser.parse_args()",
      "",
      "\tmodes = {",
      "\t\t\"parse\": args.parse,",
      "\t\t\"a\": args.a,",
      "\t\t\"b\": args.b",
      "\t}",
      "",
      "\tselected_mode_key = [k for k, v in modes.items() if v is not None]",
      "",
      "\tif not selected_mode_key:",
      "\t\tparser.print_help()",
      "\t\treturn",
      "",
      "\tmatch selected_mode_key[0]:",
      "\t\tcase \"parse\":",
      "\t\t\tpdf_name = modes[\"parse\"]",
      "\t\t\tprint(f\"Parse mode selected for {pdf_name}\")",
      "",
      "\t\tcase \"a\":",
      "\t\t\tpdf_name = modes[\"a\"]",
      "\t\t\tprint(f\"Analisi mode selected for {pdf_name}\")",
      "",
      "\t\tcase \"b\":",
      "\t\t\tpdf_paths = [f\"./out_analisi/{x}\" for x in modes[\"b\"]]",
      "\t\t\tprint(f\"Confronto emendativo tra {pdf_paths[0]} e {pdf_paths[1]}\")",
      "",
      "\t\tcase _:",
      "\t\t\tprint(\"Modalit√† non riconosciuta.\")",
      "",
      "if __name__ == '__main__':",
      "\tcli()"
    ],
    "description": "CLI multimodale con argparse e match-case"
  }
}
